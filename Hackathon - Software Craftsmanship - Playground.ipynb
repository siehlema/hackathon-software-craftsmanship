{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hackathon - Software Craftsmanship\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In diesem Notebook findet ihr alle Übungen mit relevanten Infos zum Schülerstipendium Hackathon mit dem Thema Software Craftsmanship am 20.11.2024.\n",
        "\n",
        "## Allgemeine Infos\n",
        "\n",
        "* Alle Übungsaufgaben zum Hackathon findet ihr hier. Wann welche Übung bearbeitet werden soll wird im Zuge der Präsentation klar.\n",
        "* Führt die Übungen soweit möglich im Google Colab aus. Wir können zwischendurch gerne diskutieren, wie eine Lösung in euerer IDE aussehen kann.\n",
        "* Es gibt zwei optionale Übungen. Diese könnt ihr bearbeiten, wenn ihr mit der jeweils anderen Aufgabe schneller fertig seid.\n",
        "* Denkt bei jeder Übung daran diese am Ende auch in euren Team-Branch zu mergen. Etwas Zeit dafür einplanen!\n",
        "* Bei Fragen fragt einfach unkompliziert bei uns nach\n",
        "\n",
        "Ansonsten: Viel Spaß :)"
      ],
      "metadata": {
        "id": "22eSGJsPccwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 1 - Google Colab IDE und Git\n",
        "\n",
        "* Schaut euch in diesem Notebook um\n",
        "* Was kann man hier so machen?\n",
        "* Erzeugt Text- und Code-Blöcke und führt diese aus.\n",
        "* Legt in Github einen Übungsbranch an. Speichert Changes (CTRL+S) und wählt den Übungsbranch aus. Seht euch die Changes in Github im Branch an.\n",
        "* Mergt diesen Branch über Github danach in euren Team-Branch"
      ],
      "metadata": {
        "id": "lpxU-lrMcxaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_3892qUNFk1",
        "outputId": "edcdec1b-ff7e-40ac-a6e1-17c07a9d7b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Colab Notebook Tipps\n",
        "\n",
        "* es gibt Code und Textblöcke\n",
        "* Codeblöcke könnt ihr ausführen, wenn ihr auf den Pfeil links vom Block klickt (oder schneller per CTRL+Enter)\n",
        "* Die Ausgabe wird per Default nach der Ausführung unter dem Codeblock dargestellt.\n",
        "* in Textblöcken kann man Text im Markdown Style formattieren\n",
        "* Codeblöcke sind per Default Python Code Blöcke. Man kann also direkt mit Coding loslegen.\n",
        "* Alternativ kann man in Codeblöcken auch Command Line Commands ausführen. Dazu ein `!` an den Anfang stellen.\n",
        "* schreibt man das Command `%%writefile <Dateiname>` an den Anfang eines Python Codeblocks wird der Block als Datei abgespeichert und der Python Code selbst wird NICHT ausgeführt"
      ],
      "metadata": {
        "id": "3q1ki7pujLO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 2 - Testing\n",
        "\n",
        "* erstellt einen neuen Übungsbranch\n",
        "* im Code-Block unten findet ihr die `AnnoyedToDoList`\n",
        "* schaut euch die Klasse an und führt die Beispielfunktionalität aus\n",
        "* zum Python-Testing findet ihr unten eine Erklärung mit Beispiel\n",
        "* Übungen:\n",
        "  1. teste die Funktion `add_task` mit einer Testfunktion in einer neuen Testdatei und mit passenden Assertions\n",
        "  1. die Methode `delete_task` hat noch keine Implementierung\n",
        "    * implementiere die Logik und schreibt einen Test dazu\n",
        "    * verfolge gerne die Vorgehensweise des TDD\n",
        "* mergt den Übungsbranch danach wieder in euren Teambranch"
      ],
      "metadata": {
        "id": "Q54SG7Xgdvpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the following command writes the code to an external file\n",
        "%%writefile annoyed_todo_list.py\n",
        "from typing import List\n",
        "\n",
        "def get_external_tasks():\n",
        "    # don't implement this!\n",
        "    # this function mocks the response of an external data source\n",
        "    return []\n",
        "\n",
        "class Task:\n",
        "    task_name: str\n",
        "    done: bool\n",
        "\n",
        "class AnnoyedToDoList:\n",
        "    def __init__(self):\n",
        "        self.tasks: List[Task] = []\n",
        "        self.unfinished_tasks = 0\n",
        "\n",
        "    def add_task(self, task_name):\n",
        "        \"\"\"\n",
        "        Function that adds a task to the task list\n",
        "        \"\"\"\n",
        "        if len(self.tasks) > 2:\n",
        "            print(\"Woah! That's a lot of tasks! Ever heard of work-life balance? Not doing this ..\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Yeah, alright. This time I'll add it to the list ...\")\n",
        "        new_task = Task()\n",
        "        new_task.task_name = task_name\n",
        "        new_task.done = False\n",
        "        self.tasks.append(new_task)\n",
        "        print(f\"Task '{task_name}' added.\")\n",
        "        self.unfinished_tasks += 1\n",
        "\n",
        "    def complete_task(self, task_name):\n",
        "        for task in self.tasks:\n",
        "            if task.task_name == task_name and not task.done:\n",
        "                task.done = True\n",
        "                self.unfinished_tasks -= 1\n",
        "                print(f\"Task '{task_name}' completed.\")\n",
        "                if self.unfinished_tasks == 0:\n",
        "                    print(\"Wow! You finished all your tasks. Are you sure you're not a robot?\")\n",
        "                return\n",
        "        print(f\"Task '{task_name}' not found or already completed.\")\n",
        "\n",
        "    def show_tasks(self):\n",
        "        if len(self.tasks) == 0:\n",
        "            print(\"Nothing to do! Go binge-watch a show or something.\")\n",
        "        else:\n",
        "            for i, task in enumerate(self.tasks):\n",
        "                status = \"Done\" if task.done == True else \"Not Done\"\n",
        "                print(f\"{i+1}. {task.task_name} - {status}\")\n",
        "\n",
        "    def delete_task(self, task_name):\n",
        "      for task in self.tasks[:]:  # Kopie der Liste für sichere Iteration\n",
        "        if task.task_name == task_name:\n",
        "            self.tasks.remove(task)\n",
        "            if not task.done:\n",
        "                self.unfinished_tasks -= 1\n",
        "            print(f\"Task '{task_name}' deleted. I hope you're happy now...\")\n",
        "            return\n",
        "      print(f\"Task '{task_name}' not found. Lucky you!\")\n",
        "\n",
        "\n",
        "    def clear_tasks(self):\n",
        "        self.tasks.clear()\n",
        "        print(\"All tasks cleared! Now you're free... or are you?\")\n",
        "\n",
        "    def import_tasks_from_external(self):\n",
        "        external_tasks = get_external_tasks()\n",
        "        for task in external_tasks:\n",
        "            self.add_task(task)\n",
        "        print(f\"Imported {len(external_tasks)} tasks from external source.\")"
      ],
      "metadata": {
        "id": "LK1ZxIQhzyXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66037817-7638-4fd0-8d53-9267e1222780"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting annoyed_todo_list.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from annoyed_todo_list import AnnoyedToDoList\n",
        "\n",
        "# Example usage\n",
        "todo_list = AnnoyedToDoList()\n",
        "todo_list.add_task(\"Buy groceries\")\n",
        "todo_list.add_task(\"Clean the house\")\n",
        "todo_list.show_tasks()\n",
        "todo_list.complete_task(\"Buy groceries\")\n",
        "todo_list.show_tasks()\n",
        "todo_list.clear_tasks()\n",
        "todo_list.import_tasks_from_external()"
      ],
      "metadata": {
        "id": "zixbhILjHg9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77088ee5-4d57-48e0-e9c6-d8fb402df3f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yeah, alright. This time I'll add it to the list ...\n",
            "Task 'Buy groceries' added.\n",
            "Yeah, alright. This time I'll add it to the list ...\n",
            "Task 'Clean the house' added.\n",
            "1. Buy groceries - Not Done\n",
            "2. Clean the house - Not Done\n",
            "Task 'Buy groceries' completed.\n",
            "1. Buy groceries - Done\n",
            "2. Clean the house - Not Done\n",
            "All tasks cleared! Now you're free... or are you?\n",
            "Imported 0 tasks from external source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to: Testing in Python\n",
        "\n",
        "Zum Testen verwenden wir das Standard Python Testing-Tool `pytest`."
      ],
      "metadata": {
        "id": "EVzLzj5mq3oS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wir brauchen einen Beispieltest, den wir ausführen können. Dazu folgender Python Code, welcher nach Ausführung als Datei abgespeichert wird."
      ],
      "metadata": {
        "id": "b3_EMM_8gGXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile testing_example.py\n",
        "def test_my_stuff_fails():\n",
        "    assert 42 == 43\n",
        "\n",
        "def test_my_stuff_success():\n",
        "    assert 42 == 42"
      ],
      "metadata": {
        "id": "TkHa-3p_zlzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05a2b13-dab3-4279-8cb4-78bb4161effe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing testing_example.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Ausführung von `pytest` lässt sich ganz einfach per `!pytest <Dateiname>` starten."
      ],
      "metadata": {
        "id": "k2OrsXX0sC5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest testing_example.py"
      ],
      "metadata": {
        "id": "DKIPhzEPkH1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd61fc0-9036-4eda-8964-6e5a37c95028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "testing_example.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                        [100%]\u001b[0m\n",
            "\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_______________________________________ test_my_stuff_fails ________________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_my_stuff_fails\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m \u001b[94m42\u001b[39;49;00m == \u001b[94m43\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       assert 42 == 43\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtesting_example.py\u001b[0m:2: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m testing_example.py::\u001b[1mtest_my_stuff_fails\u001b[0m - assert 42 == 43\n",
            "\u001b[31m=================================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ====================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests_uebung2_add.py\n",
        "\n",
        "#teste die Funktion add_task mit einer Testfunktion in einer neuen Testdatei und mit passenden Assertions\n",
        "from annoyed_todo_list import AnnoyedToDoList\n",
        "\n",
        "def test_add_task():\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    todo_list.add_task(\"Einkaufen\")\n",
        "    assert len(todo_list.tasks) == 1\n",
        "    assert todo_list.tasks[0].task_name == \"Einkaufen\"\n",
        "    assert todo_list.tasks[0].done == False\n",
        "    assert todo_list.unfinished_tasks == 1\n",
        "\n",
        "def test_task_limit():\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    tasks = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\"]\n",
        "    for task in tasks:\n",
        "        todo_list.add_task(task)\n",
        "\n",
        "    assert len(todo_list.tasks) == 3\n",
        "    assert todo_list.unfinished_tasks == 3"
      ],
      "metadata": {
        "id": "fGsFP42pfDGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e7a299-b939-40ac-f5c9-75e3d784d3b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests_uebung2_add.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests_uebung2_add.py"
      ],
      "metadata": {
        "id": "SBQBDFL-kVQc",
        "outputId": "6c981c8c-c5c4-479a-87c6-5a96a5f91cb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "tests_uebung2_add.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                      [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests_uebung2_delete.py\n",
        "\n",
        "from annoyed_todo_list import AnnoyedToDoList\n",
        "\n",
        "def test_unfinished_tasks_counter():\n",
        "  todo_list = AnnoyedToDoList()\n",
        "  todo_list.add_task(\"Task 1\")\n",
        "  todo_list.add_task(\"Task 2\")\n",
        "  todo_list.add_task(\"Task 3\")\n",
        "  assert len(todo_list.tasks) == 3\n",
        "  assert todo_list.unfinished_tasks == 3\n",
        "\n",
        "  todo_list.complete_task(\"Task 1\")\n",
        "  assert len(todo_list.tasks) == 3\n",
        "  assert todo_list.unfinished_tasks == 2\n",
        "\n",
        "  todo_list.delete_task(\"Task 2\")\n",
        "  assert len(todo_list.tasks) == 2\n",
        "  assert todo_list.unfinished_tasks == 1\n",
        "\n",
        "  todo_list.delete_task(\"Task 1\")\n",
        "  assert len(todo_list.tasks) == 1\n",
        "  assert todo_list.unfinished_tasks == 1\n",
        "\n",
        "\n",
        "\n",
        "def test_delete_nonexistent_task():\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    todo_list.add_task(\"Task 1\")\n",
        "\n",
        "    todo_list.delete_task(\"NonExistentTask\")\n",
        "\n",
        "    assert len(todo_list.tasks) == 1\n",
        "    assert todo_list.unfinished_tasks == 1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "64J-cksLkrW-",
        "outputId": "a2695847-6149-4c6c-df12-3c7c7308c749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests_uebung2_delete.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests_uebung2_delete.py"
      ],
      "metadata": {
        "id": "GmqetP8KkjKq",
        "outputId": "15cdd6ef-dfcf-40a3-ed75-426fc67f989f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                                                  \u001b[0m\n",
            "\n",
            "tests_uebung2_delete.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 2 (Optional) - Unit Test mit Mock\n",
        "\n",
        "* Die `AnnoyedTodoList` hat eine Funktion `import_tasks_from_external`\n",
        "* Diese ruft eine externe Funktion auf, welche aktuell immer eine leere Liste zurückliefert\n",
        "* Wir wollen jetzt so tun, als ob diese Methode eine Liste von extern bereitstellen würde\n",
        "* Schreibe dazu einen neuen Unit Test, in dem die Response der externen Methode `get_external_tasks` gemockt ist und prüfe das Ergebnis des Imports\n"
      ],
      "metadata": {
        "id": "igOpxNVKlWsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests_uebung2_optional.py\n",
        "\n",
        "from annoyed_todo_list import AnnoyedToDoList\n",
        "\n",
        "def test_import_external_tasks(monkeypatch):\n",
        "    # Mock-Daten definieren\n",
        "    mock_tasks = [\"External Task 1\", \"External Task 2\"]\n",
        "\n",
        "    # Mock-Funktion erstellen\n",
        "    def mock_get_external_tasks():\n",
        "        return mock_tasks\n",
        "\n",
        "    # get_external_tasks ersetzen\n",
        "    monkeypatch.setattr('annoyed_todo_list.get_external_tasks', mock_get_external_tasks)\n",
        "\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    todo_list.import_tasks_from_external()\n",
        "\n",
        "    assert len(todo_list.tasks) == 2\n",
        "    assert todo_list.tasks[0].task_name == \"External Task 1\"\n",
        "    assert todo_list.tasks[1].task_name == \"External Task 2\"\n",
        "    assert todo_list.unfinished_tasks == 2\n",
        "\n",
        "def test_import_empty_external_tasks(monkeypatch):\n",
        "    # Mock-Funktion für leere Liste\n",
        "    def mock_get_empty_tasks():\n",
        "        return []\n",
        "\n",
        "    monkeypatch.setattr('annoyed_todo_list.get_external_tasks', mock_get_empty_tasks)\n",
        "\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    todo_list.import_tasks_from_external()\n",
        "\n",
        "    assert len(todo_list.tasks) == 0\n",
        "    assert todo_list.unfinished_tasks == 0\n",
        "\n",
        "def test_import_respects_task_limit(monkeypatch):\n",
        "    # Mock-Daten mit mehr Tasks als erlaubt\n",
        "    mock_tasks = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\"]\n",
        "\n",
        "    def mock_get_many_tasks():\n",
        "        return mock_tasks\n",
        "\n",
        "    monkeypatch.setattr('annoyed_todo_list.get_external_tasks', mock_get_many_tasks)\n",
        "\n",
        "    todo_list = AnnoyedToDoList()\n",
        "    todo_list.import_tasks_from_external()\n",
        "\n",
        "    assert len(todo_list.tasks) == 3\n",
        "    assert todo_list.unfinished_tasks == 3"
      ],
      "metadata": {
        "id": "Jhn2WRrAq-pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8005bb-71ec-4adf-afb8-09b6ff4705cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests_uebung2_optional.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests_uebung2_optional.py"
      ],
      "metadata": {
        "id": "LKO3SsQa0Yv-",
        "outputId": "1e96a627-c62f-49a8-a8ad-617aeebd7c4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1, typeguard-4.4.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                                                  \u001b[0m\n",
            "\n",
            "tests_uebung2_optional.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 3 - Linting\n",
        "\n",
        "* erstellt einen Übungs-Branch für diese Übung aus eurem Team-Branch\n",
        "* analysiert den Code der `AnnoyedToDoList`\n",
        "* diese wurde bereits in die Datei `annoyed_todo_list.py` geschrieben\n",
        "* nutzt zur `pylint` zur Code-Analyse\n",
        " * ihr findet unten eine Erklärung mit Beispiel\n",
        " * führt die Analyse wie beschrieben aus und behebt die angegebenen Issues, soweit ihr in der Zeit kommt\n",
        "* committet die Fixes in eurem Übungs-Branch\n",
        "* mergt den Übungsbranch danach wieder in euren Team-Branch"
      ],
      "metadata": {
        "id": "RQFT-Mw-fLxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to: Linting in Python\n",
        "\n",
        "Zum Linting installieren und initialisieren wir zuerst die notwendigen Python Tools:"
      ],
      "metadata": {
        "id": "B8OncaxFqlNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylint"
      ],
      "metadata": {
        "id": "IBggQVmYU3Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Danach brauchen wir Python Code, den wir analysieren können. Dazu folgendes Beispiel, welches nach Ausführung als Datei abgespeichert wird."
      ],
      "metadata": {
        "id": "toULD6h1i1gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linting_example.py\n",
        "def square_of_number(\n",
        "     num1, num2, num3,\n",
        "     num4):\n",
        "  return num1**2, num2**2, num3**3"
      ],
      "metadata": {
        "id": "6w0Voe1viQ9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Ausführung von `pylint` ist dann ganz einfach per `!pylint <Dateiname>` machbar."
      ],
      "metadata": {
        "id": "FtRWchlhjAof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pylint linting_example.py"
      ],
      "metadata": {
        "id": "wyo3mBZFU70W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 3 (Optional & Fortgeschritten) - SOLID Prinzipien\n",
        "\n",
        "* schaut euch den Code der `AnnoyedTodoList` im Detail an\n",
        "* wie kann die Klasse umgebaut werden, sodass einige der SOLID Prinzipien eingehalten werden? Ein paar Tipps und Fragen dazu zur Anregung :)\n",
        " * Ist die Listenklasse aktuell für eine Aufgabe zuständig? Falls nein, wie könnte man das verbessern?\n",
        " * Kann das Verhalten der Liste aktuell einfach erweitert werden? Falls nein, wie könnte man das verbessern?\n",
        " * Macht evtl die Verwendung einer Basisklasse Sinn?\n",
        " * Könnten die Funktionalitäten der Liste aufgeteilt werden?\n",
        " * Wie könnte man die Verwendung der externen Methode `get_external_tasks` noch flexibler gestalten?\n",
        "* wie zuvor erstellt einen Übungs-Branch dazu und mergt ihn danach in den Team-Branch"
      ],
      "metadata": {
        "id": "ViZnnqihgQd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Übung 4 - Code Reviews\n",
        "\n",
        "* Stellt einen Pull Request ein, bei dem ihr euren Team-Branch auf den main Branch mergen wollt\n",
        "* Sucht euch ein anderes Team\n",
        "* führt jeweils gegenseitig ein Review auf diesem Pull Request durch und fügt Anmerkungen hinzu, die euch auffallen\n",
        "* das gesamte Notebook File ist als JSON formatiert. Deshalb wundert euch nicht, dass die Code Blöcke beim Review nicht als Python Code dargestellt.\n",
        "* tauscht euch danach gegenseitig aus"
      ],
      "metadata": {
        "id": "PLhdPSORgjtF"
      }
    }
  ]
}